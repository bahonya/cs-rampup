## Statistical language model
A **statistical language model** is a probability distribution over sequences of words
- Focus on statistical regularities in the generation of natural language texts
- Language models can be used to solve [[NLP]] tasks

### Word Embeddings
**Neural networks** can learn **word associations** from natural language texts automatically
- Word2vec embeds word based on their textual contexts in a high dimensional vector space
![[WE_Word2vec.png]]

### Transformer Models
A **transformer model** is a deep learning model that uses **self-attention**
- **Attention** speeds up the computation of large models by determining the most relevant context information for a word
- Transformer models like BERT or GPT3 achieve **best performance** in common [[NLP tasks]] 